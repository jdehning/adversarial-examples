machine learning, including state-of-the-art NN, are vulnerable to adversarial examples

meaning: they misclassify examples, which are only slightly different from correctly classified ones

in many cases different machine learning models trained on different subsets of training data, misclassify the same adversarial examples

thus adversarial examples seem to reveal a fundamental weakness in machine learning models

Goodfellow et al. argue that adversarial examples do not rise from extreme nonlinearity, but that they can be explained by linear behaviour
in high-dimensional spaces.